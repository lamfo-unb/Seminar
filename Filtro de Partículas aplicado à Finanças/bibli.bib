@book{Gaskell2013,
abstract = {Incorporating a broad range of qualitative and quantitative approaches, the contributors focus on good practice, clarifying procedures and public accountability, with step-by-step application, examples, advice, a glossary and further reading.},
author = {Gaskell, George and Bauer, Martin. W.},
booktitle = {The SAGE Handbook of Qualitative Data Analysis},
isbn = {9780761964810},
issn = {1982-7849},
pages = {173--191},
publisher = {Springer},
title = {{Qualitative Researching with Text , Image and Sound Discourse Analysis}},
year = {2013}
}
@article{Rajaraman2011,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
author = {Rajaraman, Anand and Ullman, Jeffrey D},
eprint = {arXiv:1011.1669v3},
isbn = {9781139058452},
issn = {01420615},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
pmid = {1107015359},
title = {{Mining of Massive Datasets}},
volume = {67},
year = {2011}
}
@article{Feinerer2008,
abstract = {During the last decade text mining has become a widely used discipline utilizing sta- tistical and machine learning methods. We present the tm package which provides a framework for text mining applications within R. We give a survey on text mining facili- ties in R and explain how typical application tasks can be carried out using our framework. We present techniques for count-based analysismethods, text clustering, text classification and string kernels.},
author = {Feinerer, Ingo and Hornik, Kurt and Meyer, David},
journal = {Journal Of Statistical Software},
keywords = {count based evaluation,r,string,text classification,text clustering,text mining},
number = {5},
pages = {1--54},
title = {{Text Mining Infrastructure in R}},
volume = {25},
year = {2008}
}
@article{Bolasco2010,
abstract = {This paper offers a concise description of the general aims of the automatic analysis of textual data (AATD) from the perspective of the statistical treatment of unstructured data in relation to structured data. An overview of the instruments, characteristics and limitations of AATD, both in the context of text analysis and of text mining, is offered. The practical results are illustrated via a case study concerning gastronomy. The study focuses on the analysis of the language of gastronomic critique; on the individuation of typologies of cuisine in Italy; on examples of fuzzy categorisation of reviews of eating places; and on the construction of grammars for the recognition of complex structures such as those used in menu entries.},
author = {Bolasco, Sergio},
journal = {Italian Journal of Applied Statistics},
keywords = {Local grammar,Meta-information,Text analysis,Text mining,Unstructured data},
number = {1},
pages = {9 -- 21},
title = {{Introduction to the Automatic Analysis of Textual Data via a Case Study}},
volume = {22},
year = {2010}
}
@book{Krippendorff1980,
abstract = {The Second Edition of Content Analysis: An Introduction to Its Methodology is a definitive sourcebook of the history and core principles of content analysis as well as an essential resource for present and future studies. The book introduces readers to ways of analyzing meaningful matter such as texts, images, voices - that is, data whose physical manifestations are secondary to the meanings that a particular population of people brings to them. Organized into three parts, the book examines the conceptual and methodological aspects of content analysis and also traces several paths through content analysis protocols. The author has completely revised and updated the Second Edition, integrating new information on computer-aided text analysis. The book also includes a practical guide that incorporates experiences in teaching and how to advise academic and commercial researchers. In addition, Krippendorff clarifies the epistemology and logic of content analysis as well as the methods for achieving its aims. Intended as a textbook for advanced undergraduate and graduate students across the social sciences, Content Analysis, Second Edition will also be a valuable resource for practitioners in a variety of disciplines.},
author = {Krippendorff, Klaus H.},
booktitle = {Education},
isbn = {978-0803914988},
issn = {01621459},
pages = {440},
publisher = {SAGE},
pmid = {370546962},
title = {{Content Analysis: An Introduction to Its Methodology}},
volume = {79},
year = {1980}
}
@article{Barberis2000,
abstract = {We examine how the evidence of predictability in asset returns affects optimal portfolio choice for investors with long horizons. Particular attention is paid to estimation risk, or uncertainty about the true values of model parameters. We find that even after incorporating parameter uncertainty, there is enough predictability in returns to make investors allocate substantially more to stocks, the longer their horizon. Moreover, the weak statistical significance of the evidence for predictabil- ity makes it important to take estimation risk into account; a long-horizon investor who ignores it may overallocate to stocks by a sizeable amount.},
author = {Barberis, Nicholas},
file = {:C$\backslash$:/Users/IgorNascimento/Desktop/alloc{\_}jnl.pdf:pdf},
isbn = {1540-6261},
issn = {00221082},
journal = {Journal of Finance},
number = {1},
pages = {225--264},
title = {{Investing for the Long Run When Returns Are Predictable}},
volume = {55},
year = {2000}
}
@article{Bialek2001,
abstract = {We de?ne predictive information Ipred(T) as the mutual information be- tween the past and the future of a time series.Three qualitatively different behaviors are found in the limit of large observation times T: Ipred(T) can remain ?nite, grow logarithmically, or grow as a fractional power law. If the time series allows us to learn a model with a ?nite number of param- eters, then Ipred(T) grows logarithmically with a coef?cient that counts the dimensionality of the model space. In contrast, power-law growth is associated, for example, with the learning of in?nite parameter (or non- parametric) models such as continuous functions with smoothness con- straints. There are connections between the predictive information and measures of complexity that have been de?ned both in learning theory and the analysis of physical systems through statistical mechanics and dynamical systems theory. Furthermore, in the same way that entropy provides the unique measure of available information consistent with some simple and plausible conditions, we argue that the divergent part of Ipred(T) provides the unique measure for the complexity of dynam- ics underlying a time series. Finally, we discuss how these ideas may be useful in problems in physics, statistics, and biology.},
author = {Bialek, William and Nemenman, Ilya and Tishby, Naftali},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
number = {1949},
pages = {2409--2463},
pmid = {11674845},
primaryClass = {physics},
title = {{Predictability , Complexity , and Learning}},
volume = {13},
year = {2001}
}
@article{Bookstaber2015,
abstract = {When one looks beyond the immediate pension plan liabilities represented by existing obligations, one sees a stream of long-term obligations that will shift as underlying economic factors such as inflation and productivity change. Thus the pension liability may be broken down into a short-term, bond-like component and a long-term, equity-like component. Plan investments need to meet the immediate obligations and avoid deterioration of the current asset/liability ratio, but they also need to maximize long-term returns in order to meet the long-term obligations. An all-equity portfolio meets the latter need, but is far too risky in the short term. A fully hedged, dedicated portfolio will ensure that short-term obligations are met, but will do little to increase asset value to meet the incremental, long-term obligations. The task confronting the pension plan sponsor is to construct the liability asset--the asset that will meet these disparate aspects of the pension liability. Doing so requires a dynamically adjusted asset mix, where equity exposure is increased as the surplus value rises and assets are shifted to an immunized portfolio to match the interestrate sensitivity of the liabilities as surplus value erodes. The resulting strategy, called surplus insurance, provides a minimum surplus floor value while allowing the highest expected return on plan assets, and therefore the greatest possibility of meeting the long-term obligations.},
author = {Bookstaber, Richard and Gold, Jeremy},
issn = {0015198X},
journal = {Financial Analysts Journal},
number = {1},
pages = {18--28},
title = {{In search of the liability asset}},
volume = {71},
year = {2015}
}
@article{Brandt2005,
abstract = {We present a simulation-based method for solving discrete-time portfolio choice problems involving non-standard preferences, a large number of assets with arbitrary return distribution, and, most importantly, a large number of state variables with potentially path-dependent or non-stationary dynamics. The method is flexible enough to accommodate intermediate consumption, portfolio constraints, parameter and model uncertainty, and learning. We first establish the properties of the method for the portfolio choice between a stock index and cash when the stock returns are either iid or predictable by the dividend yield. We then explore the problem of an investor who takes into account the predictability of returns but is uncertain about the parameters of the data generating process. The investor chooses the portfolio anticipating that future data realizations will contain useful information to learn about the true parameter values. Address correspondence to Amit Goyal, Goizueta Business School, Emory University, 1300 Clifton Rd., Atlanta, GA 30322-2722 or e-mail: Amit{\_}Goyal@bus.emory.edu.},
author = {Brandt, Michael W and Goyal, Amit and Santa-Clara, Pedro and Stroud, Jonathan R},
isbn = {08939454},
issn = {0893-9454},
journal = {Review of Financial Studies},
keywords = {COMPLETE MARKETS,CONSTRAINED OPTIMIZATION,Dubovitskii–Milyutin theorem,INCOMPLETE INFORMATION,INVESTMENT,MEAN-VARIANCE,MODEL,Neumann parabolic equations with an infinite numbe,OPTIMAL CONSUMPTION,SELECTION,STRATEGIC ASSET ALLOCATION,UTILITY,Weierstrass theorem,conical approximations,optimality conditions,time optimal control problems},
number = {January 2000},
pages = {831--873},
title = {{A Simulation Approach to Dynamic Portfolio Choice with an Application to Learning About Return Predictability}},
volume = {18},
year = {2005}
}
@article{Carvalho2010,
abstract = {Particle learning (PL) provides state filtering, sequential parameter learning and smoothing in a general class of state space models. Our approach extends existing particle methods by incorporating the estimation of static parameters via a fully-adapted filter that utilizes conditional sufficient statistics for parameters and/or states as particles. State smoothing in the presence of parameter uncertainty is also solved as a by-product of PL. In a number of examples, we show that PL outperforms existing particle filtering alternatives and proves to be a competitor to MCMC.},
archivePrefix = {arXiv},
arxivId = {1011.1098},
author = {Carvalho, Carlos M. and Johannes, Michael S. and Lopes, Hedibert F. and Polson, Nicholas G.},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Mixture Kalman filter,and phrases,learning,mixture kalman filter,parameter learning,particle,particle learning,sequential inference,smoothing,state filtering,state space models},
number = {1},
pages = {88--106},
title = {{Particle Learning and Smoothing}},
volume = {25},
year = {2010}
}
@article{Chiu2014,
abstract = {Consider an insurer who invests in the financial market where correlations among risky asset returns are randomly changing over time. The insurer who faces the risk of paying stochastic insurance claims needs to manage her asset and liability by taking into account of the correlation risk. This paper investigates the impact of correlation risk to the optimal asset-liability management (ALM) of an insurer. We employ the Wishart process to model the stochastic covariance matrix of risky asset returns. The insurer aims to minimize the variance of the terminal wealth given an expected terminal wealth subject to the risk of paying out random liabilities of compound Poisson process. This ALM problem then becomes a linear-quadratic stochastic optimal control problem with stochastic volatilities, stochastic correlations and jumps. The recognition of an affine form in the solution process enables us to derive the explicit closed-form solution to the optimal ALM portfolio policy, obtain the efficient frontier, and identify the condition that the solution is well behaved.},
author = {Chiu, Mei Choi and Wong, Hoi Ying},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
title = {{Mean-variance asset-liability management with asset correlation risk and insurance liabilities}},
volume = {59},
year = {2014}
}
@article{Chiu2013,
abstract = {Using the diffusion limit of the discrete-time error correction model of cointegration for risky assets and geometric Brownian motion for the value of liabilities, we solve the asset-liability management (ALM) problem using the theory of backward stochastic differential equations. The solutions of the ALM policy and the efficient frontier in terms of surplus are obtained as closed-form formulas. We numerically examine the impact of cointegration to the trade-off between risk and return in managing cointegrated risky assets and random liabilities. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Chiu, Mei Choi and Wong, Hoi Ying},
issn = {01676377},
journal = {Operations Research Letters},
number = {1},
title = {{Mean-variance principle of managing cointegrated risky assets and random liabilities}},
volume = {41},
year = {2013}
}
@article{Consigli1998,
author = {Consigli, G and Dempster, M a H},
doi = {10.2139/ssrn.34780},
isbn = {0043139700431397},
issn = {15565068},
journal = {Annals of Operations Research},
number = {October},
pages = {131 -- 161},
title = {{Dynamic stochastic programming for asset-liability management}},
url = {http://www.ssrn.com/abstract=34780},
volume = {81},
year = {1998}
}

@article{Consigli2012,
abstract = {Increasing financial pressure on State-controlled pension systems has caused, over the last two decades or so, an unprecedented effort by private pension funds (PFs) and insurance companies to issue new types of retirement vehicles. This article investigates the effects of such widespread phenomenon from the perspective of individual asset-liability management. A multistage stochastic programming problem has been formulated with investment opportunities including PFs, unit-linked contracts and variable life annuities. The introduction of a specific risk measure with respect to a desirable retirement income stream and a planning horizon spanning the entire individuals' working life helps to analyse the implications of observed market dynamics on retirement strategies. We present comparative results focusing on the retirement planning problem for three representative individuals carrying different time horizons but common retirement goals. The results show the benefits over traditional pension accumulation plans of dynamic strategies based on mixed portfolios of retirement products. The authors 2012. Published by Oxford University Press on behalf of the Institute of Mathematics and its Applications. All rights reserved.2012 {\textcopyright} The authors 2012. Published by Oxford University Press on behalf of the Institute of Mathematics and its Applications. All rights reserved.},
author = {Consigli, Giorgio and Iaquinta, Gaetano and Moriggia, Vittorio and {Di Tria}, Massimo and Musitelli, Davide},
issn = {14716798},
journal = {IMA Journal of Management Mathematics},
keywords = {asset-liability management,individual retirement planning,multistage stochastic programming,private pension plans,variable life annuities},
number = {4},
pages = {365--396},
title = {{Retirement planning in individual asset-liability management}},
volume = {23},
year = {2012}
}
@article{Dempster2003,
abstract = {Dynamic financial analysis (DFA) is a technique which uses Monte Carlo simulation to investigate the evolution over time of financial models of funds, complex liabilities and entire firms. Although of increasing popularity, the drawback of DFA is the dearth of systematic methods for optimising model parameters for strategic financial planning. This paper introduces strategic DFA which employs the only recently mature technology of dynamic stochastic optimisation to fill this gap. The new approach is described in terms of an illustrative case study of a joint university/industry project to create a decision support system for strategic asset liability management involving global asset classes and defined contribution pension plans. Although the application of the system described in the paper is to fund design and risk management, the approach and techniques described here are much more broadly applicable to strategic financial planning problems; for example, to insurance and reinsurance firms, to risk capital allocation in financial institutions and trading firms and to corporate investment and business development involving real options. As well as describing the mathematical and statistical models used in the case study, the paper treats econometric estimation, asset return and liability scenario generation, model specification and optimisation, system evaluation and historical backtesting. Throughout the system visualisation plays an important role.},
author = {Dempster, Michael A H and Germano, Medova and Medova, Elena A and Villaverde, Michael},
issn = {2044-0456},
journal = {British Actuarial Journal},
number = {01},
pages = {137--195},
title = {{Global asset liability management}},
volume = {9},
year = {2003}
}
@article{Ferstl2011,
abstract = {Stochastic linear programming is a suitable numerical approach for solving practical asset-liability management problems. In this paper, we consider a multi-stage setting under time-varying investment opportunities and propose a decomposition of the benefits in dynamic re-allocation and predictability effects. We use a first-order unrestricted vector autoregressive process to model asset returns and state variables and include, in addition to equity returns and dividend-price ratios, Nelson/Siegel parameters to account for the evolution of the yield curve. The objective is to minimize the Conditional Value at Risk of shareholder value, i.e., the difference between the mark-to-market value of (financial) assets and the present value of future liabilities. ?? 2010 Elsevier B.V.},
author = {Ferstl, Robert and Weissensteiner, Alex},
isbn = {0378-4266},
issn = {03784266},
journal = {Journal of Banking and Finance},
keywords = {Asset-liability management,Predictability,Scenario generation,Stochastic programming,VAR process},
number = {1},
pages = {182--192},
title = {{Asset-liability management under time-varying investment opportunities}},
volume = {35},
year = {2011}
}
@article{Gondzio2001,
abstract = {Financial institutions require sophisticated tools for risk management. For companywide risk management, both sides of the balance sheet should be considered, resulting in an integrated asset-liability management approach. Stochastic programming models suit these needs well and have already been applied in the field of asset-liability management to improve financial operations and risk management.The dynamic aspect of the financial planning problems inevitably leads to multiple decision stages (trading dates) in the stochastic program and results in an explosion of dimensionality. In this paper we show that dedicated model generation, specialized solution techniques based on decomposition and high-performance computing, are the essential elements to tackle these large-scale financial planning problems. It turns out that memory management is a major bottleneck when solving very large problems, given an efficient solution approach and a parallel computing facility. We report on the solution of an asset-liability management model for an actual Dutch pension fund with 4,826,809 scenarios; 12,469,250 constraints; and 24,938,502 variables; which is the largest stochastic linear program ever solved. A closer look at the optimal decisions reveals that the initial asset mix is more stable for larger models, demonstrating the potential benefits of the high-performance computing approach for ALM.},
annote = {Artigo cont{\'{e}}m bom modelagem.},
author = {Gondzio, Jacek and Kouwenberg, Roy},
isbn = {0030-364X},
issn = {0030364X},
journal = {Operations Research},
number = {6 (November-December 2001)},
pages = {879--891},
title = {{High-Performance Computing for Asset-Liability Management}},
volume = {49},
year = {2001}
}
@article{Gulpinar2013,
abstract = {This paper presents an asset liability management model based on robust optimization techniques. The model explicitly takes into consideration the time-varying aspect of investment opportunities. The emphasis of the proposed approach is on computational tractability and practical appeal. Computational studies with real market data study the performance of robust-optimization-based strategies, and compare it to the performance of the classical stochastic programming approach.},
author = {G{\"{u}}lpinar, Nalan and Pachamanova, Dessislava},
issn = {03784266},
journal = {Journal of Banking {\&} Finance},
number = {6},
pages = {2031--2041},
title = {{A robust optimization approach to asset-liability management under time-varying investment opportunities}},
volume = {37},
year = {2013}
}
@article{Johannes2014,
abstract = {This paper finds statistically and economically significant out-of-sample portfolio benefits for an investor who uses models of return predictability when forming optimal portfolios. Investors must account for estimation risk, and incorporate an ensemble of important features, including time-varying volatility, and time-varying expected returns driven by payout yield measures that include share repurchase and issuance. Prior research documents a lack of benefits to return predictability, and our results suggest that this is largely due to omitting time-varying volatility and estimation risk. We also document the sequential process of investors learning about parameters, state variables, and models as new data arrive.},
author = {Johannes, Michael and Korteweg, Arthur and Polson, Nicholas},
isbn = {1540-6261},
issn = {00221082},
journal = {Journal of Finance},
number = {2},
pages = {611--644},
title = {{Sequential learning, predictability, and optimal portfolio returns}},
volume = {69},
year = {2014}
}
@incollection{Kouwenberg2008,
abstract = {This chapter reviews stochastic programming models for asset and liability management (ALM). It introduces the basics of stochastic programming and formulates a canonical model for portfolio management. It elaborates the key issue of generating probabilistic data for a stochastic programming ALM system on scenario-generation methods. It discusses the performance of a stochastic programming ALM model for pension funds in conjunction with alternative scenario-generation methods. It places stochastic programming models in the context of the traditional portfolio choice literature from financial economics, and discusses its advantages and limitations. It provides a review of stochastic programming applications to ALM in several institutional settings. It hinges upon solution techniques illustrating the size of problems that are solvable with current state-of-the-art software. Stochastic programming is a powerful modelling paradigm for ALM problems. It incorporates in a common framework multiple correlated sources of risk for both the asset and liability side, takes a long time horizon perspective, accommodates different levels of risk aversion and allows for dynamic portfolio rebalancing while satisfying operational or regulatory restrictions and policy requirements. ?? 2008 Elsevier B.V. All rights reserved.},
author = {Kouwenberg, Roy and Zenios, Stavros A.},
booktitle = {Handbook of Asset and Liability Management - Set},
isbn = {9780444532480},
issn = {0377-2217},
pages = {253--303},
title = {{Stochastic Programming Models for Asset Liability Management}},
volume = {1},
year = {2008},
publisher = {incollection}
}
@article{Liu2007,
abstract = {In this article, I explicitly solve dynamic portfolio choice problems, up to the solution of an ordinary differential equation (ODE), when the asset returns are quadratic and the agent has a constant relative risk aversion (CRRA) coefficient. My solution includes as special cases many existing explicit solutions of dynamic portfolio choice problems. I also present three applications that are not in the literature. Application 1 is the bond portfolio selection problem when bond returns are described by “quadratic term structure models.” Application 2 is the stock portfolio selection problem when stock return volatility is stochastic as in Heston model. Application 3 is a bond and stock portfolio selection problem when the interest rate is stochastic and stock returns display stochastic volatility. (JEL G11)},
author = {Liu, Jun},
issn = {08939454},
journal = {Review of Financial Studies},
number = {1},
pages = {1--39},
title = {{Portfolio selection in stochastic environments}},
volume = {20},
year = {2007}
}
@article{Meder2007,
abstract = {Pension assets exist to defease the benefit promises made by plan sponsors to participants and beneficiaries—in other words, the pension liability. It follows that pension investment policies should be set in a way that explicitly integrates the exposures of the pension liability. The traditional approach to pension investing has excluded the risks of the liability, which has resulted in portfolios that may be appropriate in an asset-only framework, but that are exposed to unrewarded risk when evaluated relative to liabilities. Efficient investment policies can be designed to avoid unrewarded risk if the exposures of the liability are explicitly integrated into the investment framework.},
author = {Meder, Aeron and Staub, Renato},
journal = {Society of Actuaries},
title = {{Linking Pension Liabilities to Assets}},
year = {2007}
}
@article{Nelson1987,
abstract = {This paper introduces a parametrically parsimonious model for yield curves that has the ability to represent the shapes generally associated with yield curves: monotonic, humped, and S shaped. We find that the model explains 96{\%} of the variation in bill yields across maturities during the period 1981-83. The movement of the parameters through time reflects and confirms a change in Federal Reserve monetary policy in late 1982. The ability of the fitted curves to predict the price of the long-term Treasury bond with a correlation of .96 suggests that the model captures important attributes of the yield/maturity relation.},
author = {Nelson, Charles R. and Siegel, Andrew F.},
isbn = {00219398},
issn = {0021-9398},
journal = {The Journal of Business},
number = {4},
pages = {473--489},
title = {{Parsimonious Modeling of Yield Curves}},
volume = {60},
year = {1987}
}
@article{Quaranta2008,
abstract = {This paper deals with a portfolio selection model in which the methodologies of robust optimization are used for the minimization of the conditional value at risk of a portfolio of shares. Conditional value at risk, being in essence the mean shortfall at a specified confidence level, is a coherent risk measure which can hold account of the so called "tail risk" and is therefore an efficient and synthetic risk measure, which can overcome the drawbacks of the most famous and largely used VaR. An important feature of our approach consists in the use of techniques of robust optimization to deal with uncertainty, in place of stochastic programming as proposed by Rockafellar and Uryasev. Moreover we succeeded in obtaining a linear robust copy of the bi-criteria minimization model proposed by Rockafellar and Uryasev. We suggest different approaches for the generation of input data, with special attention to the estimation of expected returns. The relevance of our methodology is illustrated by a portfolio selection experiment on the Italian market. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Quaranta, Anna Grazia and Zaffaroni, Alberto},
issn = {03784266},
journal = {Journal of Banking and Finance},
keywords = {Coherent risk measures,Conditional value at risk,Robust optimization},
number = {10},
pages = {2046--2056},
title = {{Robust optimization of conditional value at risk and portfolio selection}},
volume = {32},
year = {2008}
}
@article{Ryan2013a,
abstract = {This review tracks the development of asset/liability management from its roots in liability management outsourcing to its most recent interpretation as a broad liability-driven investing strategy.},
author = {Ryan, Ronald J.},
journal = {Research Foundation Literature Reviews},
title = {{The Evolution of Asset/Liability Management}},
year = {2013}
}
@article{Samuelson1969,
abstract = {The article presents information on lifetime portfolio selection by dynamic stochastic programming. Most analyses of portfolio selection, whether they are of the Markowitz-Tobin mean-variance or of more general type, maximize over one period. The present lifetime model reveals that investing for many periods does not itself introduce extra tolerance for riskiness at early, or any, stages of life. The model denies the validity of the concept of businessman's risk, for isoelastic marginal utilities, in your prime of life you have the same relative risk-tolerance as toward the end of life. The "chance to recoup" and tendency for the law of large numbers to operate in the case of repeated investments is not relevant. The analysis for marginal utility with elasticity differing from that of Bernoulli provides an effective counter example, if indeed a counter example is needed to refute a gratuitous assertion.},
author = {Samuelson, Paul A},
issn = {00346535},
journal = {Review of Economics and Statistics},
keywords = {ANALYSIS of variance,BUSINESSPEOPLE,CONSUMPTION (Economics),ECONOMIC models,ECONOMICS,ELASTICITY (Economics),INVESTMENTS,LINEAR programming,STOCHASTIC programming},
number = {3},
pages = {239},
title = {{Lifetime Portfolio Selection by Dynamic Stochastic Programming}},
volume = {51},
year = {1969}
}
@article{Wachter2002,
abstract = {This paper solves, in closed form, the optimal portfolio choice problem for an investor with utility over consumption under mean-reverting returns. Previous solutions either require approximations, numerical methods, or the assumption that the investor does not consume over his lifetime. This paper breaks the impasse by assuming that markets are complete. The solution leads to a new understanding of hedging demand and of the behavior of the approximate log-linear solution. The portfolio allocation takes the form of a weighted average and is shown to be analogous to duration for coupon bonds. Through this analogy, the notion of investment horizon is extended to that of an investor who consumes at multiple points in time.},
author = {Wachter, Jessica A},
isbn = {00221090},
issn = {0022-1090},
journal = {The Journal of Financial and Quantitative Analysis},
number = {1},
pages = {63--91},
title = {{Portfolio and Consumption Decisions under Mean-Reverting Returns: An Exact Solution for Complete Markets}},
volume = {37},
year = {2002}
}
@article{Wachter2010,
abstract = {This review article describes recent literature on asset allocation, covering both static and dynamic models. The article focuses on the bond-stock decision and on the implications of return predictability. In the static setting, investors are assumed to be Bayesian, and the role of various prior beliefs and specifications of the likelihood are explored. In the dynamic setting, recursive utility is assumed, and attention is paid to obtaining analytical results when possible. Results under both full- and limited-information assumptions are discussed.},
author = {Wachter, Jessica a.},
issn = {1941-1367},
journal = {Annual Review of Financial Economics},
keywords = {portfolio choice,predictive regression,recursive utility},
number = {1},
pages = {175--206},
title = {{Asset Allocation}},
volume = {2},
year = {2010}
}
@article{Welch2008,
abstract = {Our article comprehensively reexamines the performance of variables that have been suggested by the academic literature to be good predictors of the equity premium. We find that by and large, these models have predicted poorly both in-sample (IS) and out-of-sample (OOS) for 30 years now; these models seem unstable, as diagnosed by their out-of-sample predictions and other statistics; and these models would not have helped an investor with access only to available information to profitably time the market.},
author = {Welch, Ivo and Goyal, Amit},
isbn = {08939454},
issn = {08939454},
journal = {Review of Financial Studies},
number = {4},
pages = {1455--1508},
pmid = {2417353},
title = {{A comprehensive look at the empirical performance of equity premium prediction}},
volume = {21},
year = {2008}
}
@article{Xia2001,
abstract = {This paper examines the effects of uncertainty about the stock return predictability on optimal dynamic portfolio choice in a continuous time setting for a long-horizon investor. Uncertainty about the predictive relation affects the optimal portfolio choice through dynamic learning, and leads to a state-dependent relation between the optimal portfolio choice and the investment horizon. There is substantial market timing in the optimal hedge demands, which is caused by stochastic covariance between stock return and dynamic learning. The opportunity cost of ignoring predictability or learning is found to be quite substantial.},
author = {Xia, Y H},
isbn = {0022-1082},
issn = {0022-1082},
journal = {Journal of Finance},
number = {1},
pages = {205--246},
title = {{Learning about predictability: The effects of parameter uncertainty on dynamic asset allocation}},
volume = {56},
year = {2001}
}
@article{Xu2012,
abstract = {The classifier of SVM decision tree (SVM-DT) takes advantage of both the efficient computation of the tree architecture and the high classification accuracy of SVMs. The paper proposes a new effective approach to optimize the SVM -DT classifier while presents the research on text categorization using SVM-DT classifier. In this approach, a novel separability measure is defined base on Support vector domain description (SVDD), and an improved SVMDT is proposed. Experimental results demonstrate the effectiveness and efficiency of the improved SVM decision tree.},
author = {Xu, Zhenqiang and Li, Pengwei and Wang, Yunxia},
issn = {18753892},
journal = {Physics Procedia},
pages = {1986--1991},
publisher = {Elsevier},
title = {{Text Classifier Based on an Improved SVM Decision Tree}},
volume = {33},
year = {2012}
}
@article{Li2011,
abstract = {When dealing with the high dimensions and large-scale multi-class textual data, it is commonly to ignore the semantic relation between words with the traditional feature selection method. In order to solve the problem, we introduce the categories information into the existing LDA model feature selection algorithm, and construct SVM multi-class classifier on the implicit topic-text matrix. Experimental results show that this method can improve classification accuracy and the dimensionality is reduced availably, the value of F1, Macro-F1, and Micro-F1 are obtained improvement.},
author = {Li, Kunlun and Xie, Jing and Sun, Xue and Ma, Yinghui and Bai, Hui},
issn = {18777058},
journal = {Procedia Engineering},
pages = {1963--1967},
publisher = {Elsevier},
title = {{Multi-class text categorization based on LDA and SVM}},
volume = {15},
year = {2011}
}
@article{Sun2011,
abstract = {The decision tree is a flexible and useful classification tool. But on the data with high dimensionality, it meets problems. For most of current decision tree algorithms, when splitting a node of a tree, only the “best” one feature is selected and used. Since more features are ignored, the classification accuracy is not high. To solve the problem, this paper uses a cluster tree for text categorization. Unlike familiar decision trees (e.g. CART, C4.5), clustering results are used as the splitting rule and more features are considered. Obviously, the used clustering algorithm is an very important to the cluster tree. For better performance, a text clustering algorithm is proposed to enhance the cluster tree. Experiments show that the cluster tree solves the high-dimensionality problem and outperforms C4.5 and CART on text data. Sometimes, it may do better than LibSVM, which may be the most powerful tool for text categorization.},
author = {Sun, Zhaocai and Ye, Yunming and Deng, Weiru and Huang, Zhexue},
issn = {18777058},
journal = {Procedia Engineering},
pages = {3785--3790},
publisher = {Elsevier},
title = {{A Cluster Tree Method For Text Categorization}},
volume = {15},
year = {2011}
}
@article{Wen-ge2012,
abstract = {In the paper, support vector machine is proposed in IR target recognition. Grid method is used to select the appropriate parameters of SVM to avoid over-fitting due to the choice of inappropriate parameters. We employ coal mine IR monitoring images to testify the IR target recognition ability of SVM. And features and category of the coal mine IR monitoring images are given. The experimental results illustrate that the IR target recognition accuracy of SVM is 100{\%}.Thus, SVM is an excellent IR target recognition method.},
author = {Wen-ge, Feng},
issn = {18753892},
journal = {Physics Procedia},
pages = {2138--2142},
title = {{Application of SVM Classifier in IR Target Recognition}},
volume = {24},
year = {2012}
}
@article{Sebastiani2002,
abstract = {The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm.We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation. Categories and Subject Descriptors: H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing—Indexing methods; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—Information filtering; H.3.4 [Information Storage and Retrieval]: Systems and Software—Performance evaluation (efficiency and effectiveness); I.2.6 [Artificial Intelligence]: Learning— Induction},
author = {Sebastiani, Fabrizio},
eprint = {0110053},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {1},
pages = {1--47},
pmid = {18202440},
primaryClass = {cs},
title = {{Machine learning in automated text categorization}},
volume = {34},
year = {2002}
}

@book{James2013,
abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
booktitle = {with Applications in R},
isbn = {1461471389},
issn = {1431-875X},
pages = {440},
pmid = {10911016},
publisher = {Springer},
title = {{An Introduction to Statistical Learning}},
volume = {103},
year = {2013}
}


@phdthesis{Bergo2014,
abstract = {Mahogany (Swietenia macrophylla King) is the world's most valuable tropical timber and was included on Convention of International Trade in Endangered Species of Wild Fauna and Flora (CITES) Appendix II as an endangered species in 2003 due to its extensive logging. Some timbers are very similar to mahogany, so strategies for identification are crucial to supervise its exploitation. The method usually applied with these purposes is visual anatomical identification, requiring the wood to be examined by specialists, which are scarce. Near Infrared Spectroscopy (NIRS) is an excellent alternative for wood identification since it allows rapid and non-destructive analysis. In this work, the similar species andiroba, cedar, curupix{\'{a}} and mahogany were discriminated with 100{\%} of correct classifications using handheld portable and high performance NIRS instruments and partial least squares discriminant analysis (PLS-DA). Some calibration transfer strategies were tested to allow data sharing between two high performance instruments with 100{\%} of correct classifications. Reverse methods of standardization were applied to calibration transfer between high performance and handheld portable instruments with 93 to 100{\%} of correct classifications. Calibration transfer was also used to make measurements of milled and block wood samples compatible. In this case, preprocessing and spectral range selecting resulted in models with 100{\%} of correct classifications. Using these same models, 465 mahogany milled samples from 26 different countries were identified by species with 97{\%} of correct classifications. Finally, among mahogany milled samples from Brazil, Honduras, Mexico, Peru and Venezuela, it was possible to discriminate those from the first three countries. Although it wasn't possible to separate Peru's samples from most of Venezuela's, the samples from both countries could be perfectly discriminated from the other three countries samples.},
author = {Bergo, Maria Cec{\'{i}}lia Jorge},
school = {Universidade de Bras{\'{i}}lia},
title = {{Transfer{\^{e}}ncia de calibra{\c{c}}{\~{a}}o na discrimina{\c{c}}{\~{a}}o de mogno e esp{\'{e}}cies semelhantes utilizando NIRS e PLS-DA}},
year = {2014}
}
@book{Johnson2007,
abstract = {Most of the observable phenomena in the empirical sciences are of a multivariate nature.In financial studies, assets in stock markets are observed simultaneously and their joint development is analyzed to better understand general tendencies and to track indices. In medicine recorded observations of subjects in different locations are the basis of reliable diagnoses and medication. In quantitative marketing consumer preferences are collected in order to construct models of consumer behavior. The underlying theoretical structure of these and many other quantitative studies of applied sciences is multivariate. Focussing on applications this book presents the tools and concepts of multivariate data analysis in a way that is understandable for non-mathematicians and practitioners who face statistical data analysis. In this second edition a wider scope of methods and applications of multivariate statistical analysis is introduced. All quantlets have been translated into the R and Matlab language and are made available online.},
author = {Johnson, Richard A and Wichern, Dean W},
booktitle = {New York},
isbn = {9783540722434},
issn = {00401706},
pages = {517},
title = {{Applied Multivariate Statistical Analysis}},
volume = {47},
year = {2007}
}
@book{Mingoti2007,
author = {Mingoti, Sueli Aparecida},
publisher = {Editora UFMG},
title = {{An{\'{a}}lise de dados atrav{\'{e}}s de m{\'{e}}todos de estat{\'{i}}stica multivariada: uma abordagem aplicada}},
year = {2007}
}

@article{Guilera2013,
abstract = {Meta-analysis refers to the statistical methods used in research synthesis for combining and integrating results from individual studies. The present study draws on the strengths of bibliometric methods in order to offer an overview of meta-analytic research activity in psychology, as well as to characterize its most important aspects and their evolution over time. A total of 2,874 articles published in scientific journals were identified and standard bibliometric indicators (e.g., number of articles, productivity by country, and national and international collaborations) and laws (e.g., Price's and Lotka's law) were applied to these data. The results suggest a clear upward trend not only in the number of articles published since the 1970s (with a peak of productivity in 2010), but also in both the number of authors by article (" x ¼ 2:75, SD = 1.53) and internationalization, especially since the 1990s. The interest in meta-analysis extends to many authors (n = 5,445), countries (n = 44) and scientific journals (n = 394), as well as to several areas of psy- chology that mostly fit a growing exponential model. In future studies it would be inter- esting to explore the citing behaviour and patterns in the meta-analysis literature.},
author = {Guilera, Georgina and Barrios, Maite and G{\'{o}}mez-Benito, Juana},
eprint = {1211.0719},
isbn = {0138-9130},
issn = {01389130},
journal = {Scientometrics},
keywords = {Bibliometrics,Impact,Meta-analysis,Productivity,Psychology},
number = {3},
pages = {943--954},
pmid = {17782335},
title = {{Meta-analysis in psychology: A bibliometric study}},
volume = {94},
year = {2013}
}
@article{Ibanez2013,
abstract = {The objective of this paper is to propose a cluster analysis methodology for measuring the performance of research activities in terms of productivity, visibility, quality, prestige and international collaboration. The proposed methodology is based on bibliometric techniques and permits a robust multi-dimensional cluster analysis at different levels. The main goal is to form different clusters, maximizing within-cluster homogeneity and between-cluster heterogeneity. The cluster analysis methodology has been applied to the Spanish public universities and their academic staff in the computer science area. Results show that Spanish public universities fall into four different clusters, whereas academic staff belong into six different clusters. Each cluster is interpreted as providing a characterization of research activity by universities and academic staff, identifying both their strengths and weaknesses. The resulting clusters could have potential implications on research policy, proposing collaborations and alliances among universities, supporting institutions in the processes of strategic planning, and verifying the effectiveness of research policies, among others.},
author = {Ib{\'{a}}{\~{n}}ez, Alfonso and Larra{\~{n}}aga, Pedro and Bielza, Concha},
journal = {Scientometrics},
keywords = {Academic staff,Bibliometric techniques,Cluster analysis methodology,Computer Science,Spain,Universities},
month = {dec},
number = {3},
pages = {571--600},
title = {{Cluster methods for assessing research performance: Exploring Spanish computer science}},
volume = {97},
year = {2013}
}
@article{Zhang2015,
abstract = {BigData, defined as structured and unstructured data containing images, videos, texts, audio and other forms of data collected from multiple datasets, is too big, too complex and moves too fast to analyze using traditional methods. This has given rise to a few issues that must be addressed; 1) how to analyze BigData across multiple datasets, 2) how to classify the different data forms, 3) how to identify BigData patterns based on its behaviours, 4) how to visualize BigData attributes in order to gain a better understanding of data. It is therefore necessary to establish a new framework for BigData analysis and visualization. In this paper, we have extended our previous works for classifying the BigData attributes into the „5Ws‟ dimensions based on different data behaviours. Our approach not only classifies BigData attributes for different data forms across multiple datasets, but also establishes the „5Ws‟ densities to represent the characteristics of data flow patterns. We use additional non-dimensional parallel axes in parallel coordinates to display the „5Ws‟ sending and receiving densities, which provide more analytic features for BigData analysis. The experiment shows that our approach with parallel coordinate visualization can be efficiently used for BigData analysis and visualization. {\textcopyright} 2015 ComSIS Consortium. All rights reserved.},
author = {Zhang, Jinson and Huang, Mao Lin and Meng, Zhao Peng},
issn = {18200214},
journal = {Computer Science and Information Systems},
keywords = {5Ws dimensions,BigData,Data visualization,Parallel coordinate},
number = {4},
pages = {1127--1191},
title = {{Visual analytics for BigData variety and its behaviours}},
volume = {12},
year = {2015}
}
@article{Indahl2009,
abstract = {We propose a new data compression method for estimating optimal latent variables in multi-variate classification and regression problems where more than one response variable is available. The latent variables are found according to a common innovative principle combining PLS methodology and canonical correlation analysis (CCA). The suggested method is able to extract predictive information for the latent variablesmore effectively than ordinary PLS approaches. Only simple modifications of existing PLS and PPLS algorithms are required to adopt the proposed method},
author = {Indahl, Ulf G. and Liland, Kristian Hovde and N{\ae}s, Tormod},
issn = {08869383},
journal = {Journal of Chemometrics},
keywords = {Canonical correlation analysis,Discriminant analysis,Partial least squares,Powered partial least squares,Regression with several responses},
number = {9},
pages = {495--504},
title = {{Canonical partial least squares-a unified PLS approach to classification and regression problems}},
volume = {23},
year = {2009}
}
@article{Indahl2005,
abstract = {A modification of the PLS1 algorithm is presented. Stepwise optimization over a set of candidate loading weights obtained by taking powers of the y--X correlations and X standard deviations generalizes the classical PLS1 based on y--X covariances and hence adds flexibility to the modelling. When good linear predictions can be obtained, the suggested approach often finds models with fewer and more interpretable components. Good performance is demonstrated when compared with the classical PLS1 on calibration benchmark data sets. An important part of the comparisons is managed by a novel model selection strategy. The selection is based on choosing the simplest model among those with a cross-validation error smaller than the pre-specified significance limit of a X2-statistic.},
author = {Indahl, Ulf},
isbn = {0886-9383},
issn = {08869383},
journal = {Journal of Chemometrics},
keywords = {Cross-validation,Model interpretation,Model selection,PLS1,Powers of correlations and standard deviations},
number = {1},
pages = {32--44},
title = {{A twist to partial least squares regression}},
volume = {19},
year = {2005}
}
@article{Liu2015,
abstract = {Nowadays, millions of people around the world use social networking sites to express everyday thoughts and feelings. Many researchers have tried to make use of social media to study users' online behaviors and psychological states. However, previous studies show mixed results about whether self-generated contents on Facebook reflect users' subjective well-being (SWB). This study analyzed Facebook status updates to determine the extent to which users' emotional expression predicted their SWB—specifically their self-reported satis- faction with life. It was found that positive emotional expressions on Facebook did not correlate with life satisfaction, whereas negative emotional expressions within the past 9–10 months (but not beyond) were significantly related to life satisfaction. These findings suggest that both the type of emotional expressions and the time frame of status updates determine whether emotional expressions in Facebook status updates can effectively reflect users' SWB. The findings shed light on the characteristics of online social media and improve the understanding of how user-generated contents reflect users' psychological states.},
author = {Liu, Pan and Tov, William and Kosinski, Michal and Stillwell, David J. and Qiu, Lin},
isbn = {2152-2715},
issn = {2152-2715},
journal = {Cyberpsychology, Behavior, and Social Networking},
number = {7},
pages = {373--379},
pmid = {26167835},
title = {{Do Facebook status updates reflect subjective well-being?}},
volume = {18},
year = {2015}
}
@article{Stuart2011,
abstract = {The most popular microblogging service, Twitter, has established a larg user base, in spite of numerous criticisms. This study aims to examine why thisis the case. In particular, the study develops a model of microblogging use continuance based on the theories of continuance, habit and critical mass. The model is then test via a web surveyof Twitter users and PLS path modeling. The results suggest that continued use intention is strongly determined by perceived usefulness, satisfaction and habit (R=0.454). The paper rounds off with conclusions and implications for future research and practice in this verynew area of inquiry.},
author = {Stuart, J and Barnes, S and Bohringer, M},
journal = {The Journal of Computer Information Systems},
keywords = {Twitter.,continuance,critical mass,habit,microblogging,microblogs},
number = {4},
pages = {10},
title = {{Modeling use continuance Behavior in microblogging services: The case of twitte}},
volume = {54},
year = {2011}
}

@Manual{RR,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2016},
    url = {https://www.R-project.org/},
}

@article{mkv,
author = {Markowitz, H. M.},
doi = {10.1111/j.1540-6261.1952.tb01525.x},
isbn = {1557861080},
issn = {00221082},
journal = {The Journal of Finance},
pmid = {6635932},
title = {Portfolio selection},
year = {1952}
}
@article{Laloux1999,
author = {Laloux, Laurent and Cizeau, Pierre and Bouchaud, Jean Philippe and Potters, Marc},
doi = {10.1103/PhysRevLett.83.1467},
isbn = {0031-9007},
issn = {10797114},
journal = {Physical Review Letters},
number = {7},
pages = {1467--1470},
title = {Noise dressing of financial correlation matrices},
volume = {83},
year = {1999}
}
@article{gordon,
 author               = {N. J. Gordon and D. J. Salmond and A. F. M. Smith},
 title                = {Novel approach to nonlinear/non-Gaussian Bayesian state estimation},
  journal   = {IEEE Proceedings F on Radar and Signal Processing},
  number = {140},
  pages = {107-113},
 year                 = {1993},
}
 
@article{doucet2,
 author               = {Arnaud Doucet and Adam M. Johansen},
 title                = {A Tutorial on Particle Filtering and Smoothing:Fifteen years later},
 year                 = {2008},
 journal               ={Technical report (Department of Statistics, University of British Columbia)},
}

@book{petris,
 author               = {Giovanni Petris and Sonia Petrone and Patrizia Campagnoli},
 publisher            = {Springer},
 title                = {Dynamic Linear Models with R},
 year                 = {2009},
}

@article{pitt,
 author               = {Michael K. Pitt and Neld Shephard },
 title                = {Filter via Simulation: Auxiliary Particle Filters},
 journal   = {Journal of the American Statistical Association},
number = {94} ,
pages = {590-599},
 year                 = {1999}
}
@article{Virbickaite2016,
author = {Virbickaite, Audrone and Lopes, Hedibert F and Aus{\'i}n, M. Concepci{\'o}n and Galeano, Pedro},
journal = {R{\&}R Bayesian Analysis},
number = {1},
pages = {1--28},
volume = {2},
title = {{Particle Leaning for Bayesian Non-Parametric Markov Switching Stochastic Volatility Model}},
year = {2016}
}
@article{Kim1998,
author = {Kim, Sangjoon and Shepherd, Neil and Chib, Siddhartha},
isbn = {0034-6527},
issn = {0034-6527},
journal = {Review of Economic Studies},
number = {3},
pages = {361--393},
title = {{Stochastic Volatility: Likelihood Inference and Comparison with ARCH Models}},
volume = {65},
year = {1998}
}
@article{HULL1987,
abstract = {One option-pricing problem that has hitherto been unsolved is the pricing of a European call on an asset that has a stochastic volatility. This paper examines this problem. The option price is determined in series form for the case in which the stochastic volatility is independent of the stock price. Numerical solutions are also produced for the case in which the volatility is correlated with the stock price. It is found that the Black-Scholes price frequently overprices options and that the degree of overpricing increases with the time to maturity.},
author = {Hull, John and White, Alan},
doi = {10.1111/j.1540-6261.1987.tb02568.x},
file = {:C$\backslash$:/Users/Igor/Downloads/hullwhite87.pdf:pdf},
isbn = {00221082},
issn = {15406261},
journal = {The Journal of Finance},
title = {{The Pricing of Options on Assets with Stochastic Volatilities}},
year = {1987}
}

@book{west,
 author               = {Mike West and Jeff Harrison},
 title                = {Bayesian Forecast and Dynamic Models},
 year                 = {1997},
  publisher            = {Springer-Verlag}
 }
 
@article{Johansen2008,
abstract = {The auxiliary particle filter (APF) introduced by Pitt and Shephard [Pitt, M.K., Shephard, N., 1999. Filtering via simulation: Auxiliary particle filters. J. Am. Statist. Ass. 94, 590-599] is a very popular alternative to Sequential Importance Sampling and Resampling (SISR) algorithms to perform inference in state-space models. We propose a novel interpretation of the APF as an SISR algorithm. This interpretation allows us to present simple guidelines to ensure good performance of the APF and the first convergence results for this algorithm. Additionally, we show that, contrary to popular belief, the asymptotic variance of APF-based estimators is not always smaller than those of the corresponding SISR estimators - even in the 'perfect adaptation' scenario. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Johansen, Adam M. and Doucet, Arnaud},
doi = {10.1016/j.spl.2008.01.032},
isbn = {0167-7152},
issn = {01677152},
journal = {Statistics and Probability Letters},
number = {12},
pages = {1498--1504},
title = {{A note on auxiliary particle filters}},
volume = {78},
year = {2008}
}
